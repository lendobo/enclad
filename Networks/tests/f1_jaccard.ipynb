{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reconstruction(adj_matrix, opt_precision_mat, threshold=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the reconstructed adjacency matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj_matrix : array-like, shape (p, p)\n",
    "        The original adjacency matrix.\n",
    "    opt_precision_mat : array-like, shape (p, p)\n",
    "        The optimized precision matrix.\n",
    "    threshold : float, optional\n",
    "        The threshold for considering an edge in the precision matrix. Default is 1e-5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dictionary containing precision, recall, f1_score, and jaccard_similarity.\n",
    "    \"\"\"\n",
    "    # Convert the optimized precision matrix to binary form\n",
    "    reconstructed_adj = (np.abs(opt_precision_mat) > threshold).astype(int)\n",
    "    np.fill_diagonal(reconstructed_adj, 0)\n",
    "\n",
    "    # True positives, false positives, etc.\n",
    "    tp = np.sum((reconstructed_adj == 1) & (adj_matrix == 1))\n",
    "    fp = np.sum((reconstructed_adj == 1) & (adj_matrix == 0))\n",
    "    fn = np.sum((reconstructed_adj == 0) & (adj_matrix == 1))\n",
    "    tn = np.sum((reconstructed_adj == 0) & (adj_matrix == 0))\n",
    "\n",
    "    # Precision, recall, F1 score\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    jaccard_similarity = tp / (tp + fp + fn)\n",
    "\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'jaccard_similarity': jaccard_similarity,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 30\n",
    "p = 20\n",
    "n = 500\n",
    "b = int(n / 2)\n",
    "Q = 100\n",
    "lambda_granularity = 100\n",
    "lambda_range_np = np.linspace(0.01, 0.4, lambda_granularity)\n",
    "lambda_range_wp = np.linspace(0.01, 0.4, lambda_granularity)\n",
    "\n",
    "all_mean_metrics = {}\n",
    "successful_runs_per_lambda = {}\n",
    "# sample run\n",
    "for lambda_idx in tqdm(range(len(lambda_range_np))):\n",
    "    mean_f1_score = 0\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_accuracy = 0\n",
    "    mean_jaccard_similarity = 0\n",
    "    counter = 0\n",
    "    for run in range(num_runs):\n",
    "        lambda_np, lambda_wp, opt_precision_mat, adj_matrix = synthetic_run(lambda_range_np[lambda_idx], lambda_range_wp[lambda_idx], p=p, n=n, b=b, Q=Q)\n",
    "\n",
    "        # sample evaluation\n",
    "        if np.any(opt_precision_mat != 0):\n",
    "            metrics = evaluate_reconstruction(adj_matrix, opt_precision_mat)\n",
    "            mean_f1_score += metrics['f1_score']\n",
    "            mean_precision += metrics['precision']\n",
    "            mean_recall += metrics['recall']\n",
    "            mean_accuracy += metrics['accuracy']\n",
    "            mean_jaccard_similarity += metrics['jaccard_similarity']\n",
    "\n",
    "\n",
    "            counter += 1\n",
    "    if counter == 0:\n",
    "        counter = 1\n",
    "    mean_f1_score /= counter\n",
    "    mean_precision /= counter\n",
    "    mean_recall /= counter\n",
    "    mean_accuracy /= counter\n",
    "    mean_jaccard_similarity /= counter\n",
    "    all_mean_metrics[str(f'{lambda_range_np[lambda_idx]} / {lambda_range_wp[lambda_idx]}')] = {'f1_score': mean_f1_score,\n",
    "                                                         'precision': mean_precision, 'recall': mean_recall, \n",
    "                                                         'jaccard_similarity': mean_jaccard_similarity, 'accuracy': mean_accuracy}\n",
    "    successful_runs_per_lambda[str(f'{lambda_range_np[lambda_idx]} / {lambda_range_wp[lambda_idx]}')] = counter\n",
    "\n",
    "\n",
    "for key in all_mean_metrics.keys():\n",
    "    print(key)\n",
    "    print(all_mean_metrics[key])\n",
    "    print('\\n')\n",
    "print('number of successful runs per lambda:')\n",
    "print(successful_runs_per_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
